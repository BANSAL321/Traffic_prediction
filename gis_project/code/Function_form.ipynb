{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heLlo\n"
     ]
    }
   ],
   "source": [
    "# @ written by shubham bansal:\n",
    "import pandas as pd\n",
    "import  numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "#from geopy.distance import great_circle\n",
    "#import pygmaps\n",
    "from scipy.spatial import ConvexHull\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('heLlo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Data_prepration(Data_frame, newfilename):\n",
    "                ### function defination \n",
    "                # data Given \n",
    "                #data_of_buses=pd.read_csv('/home/b/Desktop/Gis_project/Gis_project/data/356_EW/1july_356ew/150218661.csv')\n",
    "                data_of_buses=Data_frame.copy()\n",
    "                #print(data_of_buses.head())\n",
    "                # data extraction from data frame\n",
    "                #data_of_buses['Date'], data_of_buses['Time'] = data_of_buses['IST_DATE'].str.split(' ', 1).str\n",
    "                data_of_buses=data_of_buses.rename(index=str,columns={\"IST_TIME\": \"Time\"})\n",
    "              #  print(data_of_buses.head())\n",
    "                Cleaned_data=data_of_buses[['LAT','LONGITUDE','Time','Acc_distance']].copy()\n",
    "                Cleaned_data=Cleaned_data.sort_values('Time')\n",
    "                #print(data_of_buses.head())\n",
    "                #type(Cleaned_data)\n",
    "                #print(Cleaned_data.head())\n",
    "                Cleaned_data_array=Cleaned_data.as_matrix()\n",
    "                Cleaned_data_array\n",
    "                size=len(Cleaned_data_array)\n",
    "                size\n",
    "                Data_Train=pd.DataFrame(columns=['Start_Lat','Start_Longitude','Start_Time','End_Lat','End_Longitude','End_Time','Distance','Flag','Day_time'])\n",
    "                Data_Train\n",
    "\n",
    "                #time_x=00:00:00\n",
    "                S_lat=Cleaned_data_array[0][0]\n",
    "                S_long=Cleaned_data_array[0][1]\n",
    "                S_time=Cleaned_data_array[0][2]\n",
    "                \n",
    "                E_lat=Cleaned_data_array[0][0]\n",
    "                E_long=Cleaned_data_array[0][1]\n",
    "                E_time=Cleaned_data_array[0][2]\n",
    "                \n",
    "                Acc_start=Cleaned_data_array[0][3]\n",
    "                Acc_end=Cleaned_data_array[0][3]\n",
    "                FMT = '%H:%M:%S'\n",
    "                time_diff=0\n",
    "                flag=0\n",
    "                for i in range(1,size):\n",
    "                       # time1=(Cleaned_data_array[i][2])\n",
    "                        time2=(Cleaned_data_array[i][2])\n",
    "                        tdelta = abs(datetime.strptime(S_time, FMT) - datetime.strptime(time2, FMT))\n",
    "                        #print(tdelta)\n",
    "                        time_diff=tdelta.total_seconds()\n",
    "                       # print(time_diff)\n",
    "                       \n",
    "                        if time_diff>600 :\n",
    "                            Acc_end=Cleaned_data_array[i-1][3]\n",
    "                            E_time= Cleaned_data_array[i-1][2]\n",
    "                            E_lat=Cleaned_data_array[i-1][0]\n",
    "                            E_long=Cleaned_data_array[i-1][1]\n",
    "                            diff=Acc_end-Acc_start  \n",
    "                            #Actual_time_difference=E_time-S_time\n",
    "                            Actual_time_difference= datetime.strptime(E_time, FMT)-datetime.strptime(S_time, FMT) \n",
    "                         #   print(S_time,E_time)\n",
    "                            #print(Actual_time_difference)\n",
    "                            Actual_time_difference_total= Actual_time_difference.total_seconds()\n",
    "                           # print(Actual_time_difference_total)\n",
    "                            #print(\" helllo\")\n",
    "                            threshold_distance=(Actual_time_difference_total*(2.78))\n",
    "                            if  diff!=0:\n",
    "                                    if diff<threshold_distance :\n",
    "                                        leng=len(Data_Train)\n",
    "                                        if leng==0 :\n",
    "                                             # Day_time=(datetime.strptime(S_time, FMT)-datetime.strptime(time_x, FMT))\n",
    "                                             # Day_time_total=Day_time.total_seconds()\n",
    "                                              s=S_time\n",
    "                                              Total_day_time=sum(int(i) * 60**index for index, i in enumerate(s.split(\":\")[::-1])) \n",
    "                                            #  print(Total_day_time)\n",
    "                                              Data_Train=Data_Train.append({'Start_Lat':S_lat,'Start_Longitude':S_long,'Start_Time':S_time,'End_Lat':E_lat,'End_Longitude':E_long,'End_Time':E_time,'Distance':diff,'Flag':flag,'Day_time':Total_day_time},ignore_index=True)\n",
    "                                        else :\n",
    "                                           # print (leng)\n",
    "                                            #lat=(int)((Data_Train.iloc[leng-1][3])*1000000)\n",
    "                                          #  print (lat)\n",
    "                                           #long=(int)((Data_Train.iloc[leng-1][4])*1000000)\n",
    "                                            #if lat==(int)(S_lat*1000000) and long ==(int)(S_long*1000000) :\n",
    "                                           # if ((Data_Train.iloc[leng-1][5])==S_time)\n",
    "                                            if(True):\n",
    "                                                current_time=S_time\n",
    "                                                prvious_time=Data_Train.iloc[leng-1][5]\n",
    "                                                #print(current_time,prvious_time)\n",
    "                                                #Day_time=(datetime.strptime(Start_time, FMT)).total_seconds()\n",
    "                                                \n",
    "                                              #  Day_time=(datetime.strptime(S_time, FMT))\n",
    "                                               # Day_time_total=Day_time.total_seconds()\n",
    "                                                time_differ = (abs(datetime.strptime(current_time, FMT) - datetime.strptime(prvious_time, FMT))).total_seconds()\n",
    "                                                #(Data_Train.iloc[leng-1][5])\n",
    "                                                #print(current_time,prvious_time,time_differ)\n",
    "                                                if(time_differ<30):\n",
    "                                                    flag=flag+1\n",
    "                                                else:\n",
    "                                                    flag=0\n",
    "                                            s=S_time\n",
    "                                            Total_day_time=sum(int(i) * 60**index for index, i in enumerate(s.split(\":\")[::-1])) \n",
    "                                           # print(Total_day_time)\n",
    "                                            Data_Train=Data_Train.append({'Start_Lat':S_lat,'Start_Longitude':S_long,'Start_Time':S_time,'End_Lat':E_lat,'End_Longitude':E_long,'End_Time':E_time,'Distance':diff,'Flag':flag,'Day_time':Total_day_time},ignore_index=True)\n",
    "\n",
    "                            S_lat=Cleaned_data_array[i][0]\n",
    "                            S_long=Cleaned_data_array[i][1]\n",
    "                            S_time=Cleaned_data_array[i][2]\n",
    "                            Acc_start=Cleaned_data_array[i-1][3]\n",
    "                Data_Train.to_csv(newfilename,index=False)\n",
    "                return Data_Train\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Load_file(filename, newfilename):\n",
    "    Data_Train=pd.read_csv(filename)\n",
    "    Data_Train_final=Data_prepration(Data_Train, newfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356_EW_6july_150218661.csv\n",
      "356_EW_6july_150222706.csv\n",
      "356_EW_6july_150220682.csv\n",
      "356_EW_6july_150220615.csv\n",
      "356_EW_10july_150218661.csv\n",
      "356_EW_10july_150222706.csv\n",
      "356_EW_10july_150220682.csv\n",
      "356_EW_10july_150220615.csv\n",
      "356_EW_8july_150218661.csv\n",
      "356_EW_8july_150222706.csv\n",
      "356_EW_8july_150220682.csv\n",
      "356_EW_8july_150220615.csv\n",
      "356_EW_16july_150218661.csv\n",
      "356_EW_16july_150222706.csv\n",
      "356_EW_16july_150220682.csv\n",
      "356_EW_16july_150220615.csv\n",
      "356_EW_15july_150218661.csv\n",
      "356_EW_15july_150222706.csv\n",
      "356_EW_15july_150220682.csv\n",
      "356_EW_15july_150220615.csv\n",
      "356_EW_31july_150218661.csv\n",
      "356_EW_31july_150222706.csv\n",
      "356_EW_31july_150220682.csv\n",
      "356_EW_31july_150220615.csv\n",
      "356_EW_17july_150218661.csv\n",
      "356_EW_17july_150222706.csv\n",
      "356_EW_17july_150220682.csv\n",
      "356_EW_17july_150220615.csv\n",
      "356_EW_20july_150218661.csv\n",
      "356_EW_20july_150222706.csv\n",
      "356_EW_20july_150220682.csv\n",
      "356_EW_20july_150220615.csv\n",
      "356_EW_4july_150218661.csv\n",
      "356_EW_4july_150222706.csv\n",
      "356_EW_4july_150220682.csv\n",
      "356_EW_4july_150220615.csv\n",
      "356_EW_5july_150218661.csv\n",
      "356_EW_5july_150222706.csv\n",
      "356_EW_5july_150220682.csv\n",
      "356_EW_5july_150220615.csv\n",
      "356_EW_29july_150218661.csv\n",
      "356_EW_29july_150222706.csv\n",
      "356_EW_29july_150220682.csv\n",
      "356_EW_29july_150220615.csv\n",
      "356_EW_26july_150218661.csv\n",
      "356_EW_26july_150222706.csv\n",
      "356_EW_26july_150220682.csv\n",
      "356_EW_26july_150220615.csv\n",
      "356_EW_18july_150218661.csv\n",
      "356_EW_18july_150222706.csv\n",
      "356_EW_18july_150220682.csv\n",
      "356_EW_18july_150220615.csv\n",
      "356_EW_3july_150218661.csv\n",
      "356_EW_3july_150222706.csv\n",
      "356_EW_3july_150220682.csv\n",
      "356_EW_3july_150220615.csv\n",
      "356_EW_25july_150218661.csv\n",
      "356_EW_25july_150222706.csv\n",
      "356_EW_25july_150220682.csv\n",
      "356_EW_25july_150220615.csv\n",
      "356_EW_24july_150218661.csv\n",
      "356_EW_24july_150222706.csv\n",
      "356_EW_24july_150220682.csv\n",
      "356_EW_24july_150220615.csv\n",
      "356_EW_21july_150218661.csv\n",
      "356_EW_21july_150222706.csv\n",
      "356_EW_21july_150220682.csv\n",
      "356_EW_21july_150220615.csv\n",
      "356_EW_9july_150218661.csv\n",
      "356_EW_9july_150222706.csv\n",
      "356_EW_9july_150220682.csv\n",
      "356_EW_9july_150220615.csv\n",
      "356_EW_12july_150218661.csv\n",
      "356_EW_12july_150222706.csv\n",
      "356_EW_12july_150220682.csv\n",
      "356_EW_12july_150220615.csv\n",
      "356_EW_28july_150218661.csv\n",
      "356_EW_28july_150222706.csv\n",
      "356_EW_28july_150220682.csv\n",
      "356_EW_28july_150220615.csv\n",
      "356_EW_27july_150218661.csv\n",
      "356_EW_27july_150222706.csv\n",
      "356_EW_27july_150220682.csv\n",
      "356_EW_27july_150220615.csv\n",
      "356_EW_19july_150218661.csv\n",
      "356_EW_19july_150222706.csv\n",
      "356_EW_19july_150220682.csv\n",
      "356_EW_19july_150220615.csv\n",
      "356_EW_1july_150218661.csv\n",
      "356_EW_1july_150222706.csv\n",
      "356_EW_1july_150220682.csv\n",
      "356_EW_1july_150220615.csv\n",
      "356_EW_22july_150218661.csv\n",
      "356_EW_22july_150222706.csv\n",
      "356_EW_22july_150220682.csv\n",
      "356_EW_22july_150220615.csv\n",
      "356_EW_2july_150218661.csv\n",
      "356_EW_2july_150222706.csv\n",
      "356_EW_2july_150220682.csv\n",
      "356_EW_2july_150220615.csv\n",
      "356_EW_14july_150218661.csv\n",
      "356_EW_14july_150222706.csv\n",
      "356_EW_14july_150220682.csv\n",
      "356_EW_14july_150220615.csv\n",
      "356_EW_7july_150218661.csv\n",
      "356_EW_7july_150222706.csv\n",
      "356_EW_7july_150220682.csv\n",
      "356_EW_7july_150220615.csv\n",
      "356_EW_.ipynb_checkpoints_Untitled-checkpoint.ipynb\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-24ed0d9bc24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m#  print(os.path.join(subdir,filename),newfilename)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mLoad_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3d7cf8407d30>\u001b[0m in \u001b[0;36mLoad_file\u001b[0;34m(filename, newfilename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoad_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mData_Train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mData_Train_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mData_prepration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas/_libs/parsers.c:10862)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas/_libs/parsers.c:11138)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas/_libs/parsers.c:11884)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows (pandas/_libs/parsers.c:11755)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error (pandas/_libs/parsers.c:28765)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = '/home/b/Desktop/Traffic_prediction/gis_project/Data_route_wise/356_EW'\n",
    "for subdir, dirnames, filenames in os.walk(root):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('-checkpoint.csv') or filename.startswith('2016'):\n",
    "            continue\n",
    "#         print(subdir, filename)\n",
    "        folders = subdir.split(sep='/')\n",
    "        newfilename = folders[7]+'_'+folders[8]+'_'+filename\n",
    "      #  print(os.path.join(subdir,filename),newfilename)\n",
    "        print(newfilename)\n",
    "        Load_file(os.path.join(subdir,filename), newfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     Data_Train=pd.read_csv('/home/b/Desktop/Gis_project/Gis_project/data/356_EW/5july_356ew/150222706.csv')\n",
    "#     Data_Train_final=Data_prepration(Data_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def Data_prepration(Data_frame):\n",
    "#                 ### function defination \n",
    "#                 # data Given \n",
    "#                 #data_of_buses=pd.read_csv('/home/b/Desktop/Gis_project/Gis_project/data/356_EW/1july_356ew/150218661.csv')\n",
    "#                 data_of_buses=Data_frame.copy()\n",
    "#                 #print(data_of_buses.head())\n",
    "#                 # data extraction from data frame\n",
    "#                 #data_of_buses['Date'], data_of_buses['Time'] = data_of_buses['IST_DATE'].str.split(' ', 1).str\n",
    "#                 data_of_buses=data_of_buses.rename(index=str,columns={\"IST_TIME\": \"Time\"})\n",
    "#               #  print(data_of_buses.head())\n",
    "#                 Cleaned_data=data_of_buses[['LAT','LONGITUDE','Time','Acc_distance']].copy()\n",
    "#                 #type(Cleaned_data)\n",
    "#                 Cleaned_data.head()\n",
    "#                 Cleaned_data_array=Cleaned_data.as_matrix()\n",
    "#                 Cleaned_data_array\n",
    "#                 size=len(Cleaned_data_array)\n",
    "#                 size\n",
    "#                 Data_Train=pd.DataFrame(columns=['Start_Lat','Start_Longitude','Start_Time','End_Lat','End_Longitude','End_Time','Distance','Flag'])\n",
    "#                 Data_Train\n",
    "\n",
    "\n",
    "#                 S_lat=Cleaned_data_array[0][0]\n",
    "#                 S_long=Cleaned_data_array[0][1]\n",
    "#                 S_time=Cleaned_data_array[0][2]\n",
    "#                 E_lat=Cleaned_data_array[0][0]\n",
    "#                 E_long=Cleaned_data_array[0][1]\n",
    "#                 E_time=Cleaned_data_array[0][2]\n",
    "#                 Acc_start=Cleaned_data_array[0][3]\n",
    "#                 Acc_end=Cleaned_data_array[0][3]\n",
    "#                 FMT = '%H:%M:%S'\n",
    "#                 time_diff=0\n",
    "#                 flag=1\n",
    "#                 for i in range(1,size):\n",
    "#                        # time1=(Cleaned_data_array[i][2])\n",
    "#                         time2=(Cleaned_data_array[i][2])\n",
    "#                         tdelta = abs(datetime.strptime(S_time, FMT) - datetime.strptime(time2, FMT))\n",
    "#                         time_diff=tdelta.total_seconds()\n",
    "#                         if time_diff>600 :\n",
    "#                             Acc_end=Cleaned_data_array[i-1][3]\n",
    "#                             E_time= Cleaned_data_array[i-1][2]\n",
    "#                             E_lat=Cleaned_data_array[i-1][0]\n",
    "#                             E_long=Cleaned_data_array[i-1][1]\n",
    "#                             diff=Acc_end-Acc_start\n",
    "#                             if diff==0 :\n",
    "#                                 S_lat=Cleaned_data_array[i-1][0]\n",
    "#                                 S_long=Cleaned_data_array[i-1][1]\n",
    "#                                 S_time=Cleaned_data_array[i-1][2]\n",
    "#                                 Acc_start=Cleaned_data_array[i-1][3]\n",
    "#                             else :\n",
    "#                                     if diff<1667:\n",
    "#                                         leng=len(Data_Train)\n",
    "#                                         if leng==0 :\n",
    "#                                               Data_Train=Data_Train.append({'Start_Lat':S_lat,'Start_Longitude':S_long,'Start_Time':S_time,'End_Lat':E_lat,'End_Longitude':E_long,'End_Time':E_time,'Distance':diff,'Flag':flag},ignore_index=True)\n",
    "#                                         else :\n",
    "#                                            # print (leng)\n",
    "#                                             lat=(int)((Data_Train.iloc[leng-1][3])*1000000)\n",
    "#                                           #  print (lat)\n",
    "#                                             long=(int)((Data_Train.iloc[leng-1][4])*1000000)\n",
    "#                                             if lat==(int)(S_lat*1000000) and long ==(int)(S_long*1000000) :\n",
    "#                                                 flag=flag+1\n",
    "#                                             else:\n",
    "#                                                 flag=1\n",
    "#                                             Data_Train=Data_Train.append({'Start_Lat':S_lat,'Start_Longitude':S_long,'Start_Time':S_time,'End_Lat':E_lat,'End_Longitude':E_long,'End_Time':E_time,'Distance':diff,'Flag':flag},ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#                                     S_lat=Cleaned_data_array[i-1][0]\n",
    "#                                     S_long=Cleaned_data_array[i-1][1]\n",
    "#                                     S_time=Cleaned_data_array[i-1][2]\n",
    "#                                     Acc_start=Cleaned_data_array[i-1][3]\n",
    "#                 Data_Train.to_csv(\"Data_train_buses_5_150222706.csv\",index=False)\n",
    "#                 return Data_Train\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
